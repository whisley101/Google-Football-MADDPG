{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b673880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "import argparse\n",
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "from argparse import RawTextHelpFormatter\n",
    "\n",
    "import gfootball.env as football_env\n",
    "import gym\n",
    "import numpy as np\n",
    "import ray\n",
    "import ray.cloudpickle as cloudpickle\n",
    "import torch as T\n",
    "from gfootball import env as fe\n",
    "from ray import tune\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.spaces.space_utils import flatten_to_single_ndarray\n",
    "from ray.tune.registry import get_trainable_cls, register_env\n",
    "from rldm.utils import football_tools as ft\n",
    "from rldm.utils import gif_tools as gt\n",
    "from rldm.utils import system_tools as st\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from collections import deque\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18f63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = '3_vs_3_auto_GK'\n",
    "env = ft.RllibGFootball(env_name,write_video=False, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4da28ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a class Replay buffer with methods to store new experiences and randomly sample\n",
    "!rm -rf /tmp/football/*\n",
    "\n",
    "class Replay_Buffer():\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.replay_buffer = deque(maxlen = self.buffer_size)\n",
    "\n",
    "        \n",
    "    def new_experience(self, player_0_obs, player_1_obs, player_0_r, player_1_r, player_0_action, \\\n",
    "                       player_1_action, player_0_next_obs, player_1_next_obs,terms):\n",
    "        \n",
    "        #Add the new experience to the deque list\n",
    "        self.replay_buffer.append([player_0_obs, player_1_obs, player_0_r, player_1_r, player_0_action, \\\n",
    "                       player_1_action, player_0_next_obs, player_1_next_obs,terms])\n",
    "        \n",
    "    def sample_experiences(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        random_sample = random.sample(self.replay_buffer,self.batch_size)\n",
    "        return random_sample\n",
    "    \n",
    "    def num_experiences(self):\n",
    "        return len(self.replay_buffer)\n",
    "\n",
    "    \n",
    "#The MADDPG for this application will have 4 total neural networks. One for actor and one for critic. Each actor and critic /\n",
    "#has a target and learning NN\n",
    "    \n",
    "#This is the actor NN class. Takes in local observations and returns what action should be taken\n",
    "class ActorNN(nn.Module):\n",
    "    def __init__(self, local_observation_length, action_space_length, hidden_layer_size):\n",
    "        super(ActorNN,self).__init__()\n",
    "        #Base case is 2 hidden layers\n",
    "        self.L1 = nn.Linear(local_observation_length,hidden_layer_size)\n",
    "        self.L23 = nn.Linear(hidden_layer_size,hidden_layer_size)\n",
    "        self.L4 = nn.Linear(hidden_layer_size, action_space_length)\n",
    "        self.relu = T.nn.ReLU()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(),lr = 0.01)\n",
    "        \n",
    "    def forward(self, local_observations):\n",
    "        output = self.L1(local_observations)\n",
    "        output = self.relu(output)\n",
    "        output = self.L23(output)\n",
    "        output = self.relu(output)\n",
    "        \n",
    "        #added to enable the gumbel_softmax\n",
    "        self.out_fn = lambda x: x\n",
    "        \n",
    "        return self.out_fn(self.L4(output))\n",
    "   \n",
    "    #a different forward just for choosing action\n",
    "#     def forward_action(self, local_observations):\n",
    "#         output = self.L1(local_observations)\n",
    "#         output = self.relu(output)\n",
    "#         output = self.L23(output)\n",
    "#         output = self.relu(output)\n",
    "        \n",
    "#         #added to enable the gumbel_softmax\n",
    "#         self.out_fn = lambda x: x\n",
    "        \n",
    "#         return self.out_fn(self.L4(output))\n",
    "\n",
    "#Define the critic class that for each agent takes in the entire observation space and the actions that both agents are doing /\n",
    "#and outputs a single value for what the value is\n",
    "class CriticNN(nn.Module):\n",
    "    def __init__(self,full_observation_length,action_space_length, hidden_layer_size):\n",
    "        super(CriticNN,self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(full_observation_length + action_space_length, hidden_layer_size)\n",
    "        self.l23 = nn.Linear(hidden_layer_size,hidden_layer_size)\n",
    "        self.l4 = nn.Linear(hidden_layer_size, 1)\n",
    "        self.relu = T.nn.ReLU()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr = 0.01)\n",
    "        \n",
    "    def forward(self, observations, actions):\n",
    "        output = self.l1(T.cat([observations,actions],dim=1))\n",
    "        output = self.relu(output)\n",
    "        output = self.l23(output)\n",
    "        output = self.relu(output)\n",
    "        \n",
    "        return self.l4(output)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0169a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a class for a football player \n",
    "\n",
    "class Player():\n",
    "    def __init__(self, gamma, tau, n_actions):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.Actor_trainer = ActorNN(43,19,64)\n",
    "        self.Actor_target = ActorNN(43,19,64)\n",
    "        self.Critic_trainer = CriticNN(43,19*2,64)\n",
    "        self.Critic_target = CriticNN(43,19*2,64)\n",
    "        \n",
    "        self.update_network_parameters(tau=1)\n",
    "        \n",
    "    def choose_action(self, local_observation): #takes in an np.array of the local observations for that agent\n",
    "        \n",
    "        local_observation = T.tensor([local_observation],dtype=T.float)\n",
    "        action = self.Actor_trainer.forward(local_observation).detach()\n",
    "#         exploration = T.rand(self.n_actions) #inject some noise into the action selection process \n",
    "#         action = action + exploration\n",
    "        \n",
    "        #added for gumbel soft_max\n",
    "        action = F.gumbel_softmax(action, hard=True)\n",
    "        action = action.detach().cpu().numpy()[0]\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        target_actor_params = self.Actor_target.named_parameters()\n",
    "        actor_params = self.Actor_trainer.named_parameters()\n",
    "\n",
    "        target_actor_state_dict = dict(target_actor_params)\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        for name in actor_state_dict:\n",
    "            actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
    "                    (1-tau)*target_actor_state_dict[name].clone()\n",
    "\n",
    "        self.Actor_target.load_state_dict(actor_state_dict)\n",
    "\n",
    "        target_critic_params = self.Critic_target.named_parameters()\n",
    "        critic_params = self.Critic_trainer.named_parameters()\n",
    "\n",
    "        target_critic_state_dict = dict(target_critic_params)\n",
    "        critic_state_dict = dict(critic_params)\n",
    "        for name in critic_state_dict:\n",
    "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
    "                    (1-tau)*target_critic_state_dict[name].clone()\n",
    "\n",
    "        self.Critic_target.load_state_dict(critic_state_dict)\n",
    "    \n",
    "#define a class for the main MADDPG learning process\n",
    "\n",
    "class MADDPG_learning():\n",
    "    def __init__(self, num_players, num_samples, tau = 0.01, gamma = 0.95):\n",
    "        \n",
    "        self.num_players = num_players\n",
    "        self.players = []\n",
    "        self.batch_size = num_samples #how many samples to grab when getting memory\n",
    "        \n",
    "        #create a list of the two player class. one for player 0 and one for player 1\n",
    "        for i in range(self.num_players):\n",
    "            self.players.append(Player(gamma=gamma, tau=tau, n_actions = 19))\n",
    "        \n",
    "    def choosing_actions(self, total_observations):\n",
    "        #total observations are the observations from both player's perspective. Just feed the total observation dictionary into it\n",
    "        actions = []\n",
    "        self.total_observations = total_observations\n",
    "        self.player_0_observation = self.total_observations['player_0']\n",
    "        self.player_1_observation = self.total_observations['player_1']\n",
    "\n",
    "        for player_idx, player in enumerate(self.players):\n",
    "            if player_idx==0: #player 0\n",
    "                actions.append(player.choose_action(self.player_0_observation))\n",
    "\n",
    "            if player_idx==1: #player 1\n",
    "                actions.append(player.choose_action(self.player_1_observation))\n",
    "        \n",
    "        return actions #will be a list of integers\n",
    "\n",
    "    #pass in the current replay_buffer as it stands.\n",
    "    def MDDPG_learn(self, replay_buffer):\n",
    "#         self.num_experiences = len(replay_buffer)\n",
    "\n",
    "#             self.replay_buffer.append([player_0_obs, player_1_obs, player_0_r, player_1_r, player_0_action, \\\n",
    "#                        player_1_action, player_0_next_obs, player_1_next_obs,terms])\n",
    "\n",
    "        training_sample = replay_buffer.sample_experiences(self.batch_size)\n",
    "\n",
    "        player_0_obs = T.tensor([item[0] for item in training_sample], dtype=T.float)\n",
    "        player_1_obs = T.tensor([item[1] for item in training_sample], dtype=T.float)\n",
    "        player_0_r = T.tensor([item[2] for item in training_sample], dtype=T.float)\n",
    "        player_1_r = T.tensor([item[3] for item in training_sample], dtype=T.float)\n",
    "        player_0_action = T.tensor([item[4] for item in training_sample], dtype=T.float)\n",
    "        player_1_action = T.tensor([item[5] for item in training_sample], dtype=T.float)\n",
    "        player_0_next_obs = T.tensor([item[6] for item in training_sample], dtype=T.float)\n",
    "        player_1_next_obs = T.tensor([item[7] for item in training_sample], dtype=T.float)\n",
    "        terminals = T.tensor([item[8] for item in training_sample], dtype=T.float)\n",
    "\n",
    "        #update the critic NN and Actor NN for each player\n",
    "\n",
    "        #Get the new states the experiences arrived at, determine the action the critics would take for each agent\n",
    "        for player_idx, player in enumerate(self.players):\n",
    "            if player_idx==0: #player 0\n",
    "                #ActorNN class takes in 43 and outputs 19\n",
    "                new_action_0 = hot_shot(player.Actor_target.forward(player_0_next_obs).detach())\n",
    "#                 pol_action_0 = player.Actor_trainer.forward(player_0_obs).detach()\n",
    "                pol_action_0_g = gumbel_rumbel(player.Actor_trainer.forward(player_0_obs))\n",
    "                pol_action_0_hs = hot_shot(player.Actor_trainer.forward(player_0_obs))\n",
    "\n",
    "            if player_idx==1: #player 1\n",
    "                new_action_1 = hot_shot(player.Actor_target.forward(player_1_next_obs).detach())\n",
    "#                 pol_action_1 = player.Actor_trainer.forward(player_1_obs).detach()\n",
    "                pol_action_1_g = gumbel_rumbel(player.Actor_trainer.forward(player_1_obs))\n",
    "                pol_action_1_hs = hot_shot(player.Actor_trainer.forward(player_1_obs))\n",
    "                \n",
    "        #Concatenate together the new actions that each of the \n",
    "        new_actions = T.cat([new_action_0,new_action_1], dim=1)\n",
    "        old_actions = T.cat([player_0_action,player_1_action], dim=1)\n",
    "#         pol_actions_0 = T.cat([pol_action_0,pol_action_1], dim=1)\n",
    "        pol_actions_0 = T.cat([pol_action_0_g,pol_action_1_hs], dim=1)\n",
    "        pol_actions_1 = T.cat([pol_action_0_hs,pol_action_1_g], dim=1)\n",
    "\n",
    "        #now update each players CriticNN\n",
    "        for player_idx, player in enumerate(self.players):\n",
    "\n",
    "            #Use player_0's observation as the \"global\" observation\n",
    "            #Take the \"global\" observations and all actions as input for critic\n",
    "            critic_value_ = player.Critic_target.forward(player_0_next_obs, new_actions).flatten()\n",
    "            critic_value = player.Critic_trainer.forward(player_0_obs, old_actions).flatten()\n",
    "#             critic_value_[terminals[:,0]] = 0.0\n",
    "\n",
    "            if player_idx==0: #player 0\n",
    "                target = player_0_r + player.gamma*critic_value_*(1-terminals)\n",
    "                critic_loss = F.mse_loss(target,critic_value)\n",
    "                player.Critic_trainer.optimizer.zero_grad()\n",
    "                critic_loss.backward(retain_graph=True)\n",
    "                player.Critic_trainer.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if player_idx==1: #player 1\n",
    "                target = player_1_r + player.gamma*critic_value_*(1-terminals)\n",
    "                critic_loss = F.mse_loss(target,critic_value)\n",
    "                player.Critic_trainer.optimizer.zero_grad()\n",
    "#                 T.autograd.set_detect_anomaly(True)\n",
    "                critic_loss.backward(retain_graph=True)\n",
    "                player.Critic_trainer.optimizer.step()\n",
    "                #Finished updating the Critic NN\n",
    "\n",
    "            #Now update the Actor NN\n",
    "            if player_idx==0:\n",
    "            \n",
    "                actor_loss = player.Critic_trainer.forward(player_0_obs, pol_actions_0).flatten()\n",
    "                actor_loss = -T.mean(actor_loss)\n",
    "                player.Actor_trainer.optimizer.zero_grad()\n",
    "                actor_loss.backward(retain_graph=True)\n",
    "                player.Actor_trainer.optimizer.step()\n",
    "                \n",
    "            if player_idx==0:\n",
    "                \n",
    "                actor_loss = player.Critic_trainer.forward(player_0_obs, pol_actions_1).flatten()\n",
    "                actor_loss = -T.mean(actor_loss)\n",
    "                player.Actor_trainer.optimizer.zero_grad()\n",
    "                actor_loss.backward(retain_graph=True)\n",
    "                player.Actor_trainer.optimizer.step()\n",
    "                \n",
    "\n",
    "\n",
    "            #Now update the Actor and Critic target networks like in the MADDPG paper gradually using tau\n",
    "            player.update_network_parameters()\n",
    "\n",
    "#takes in the logit output tensor and outputs tensor with argmax\n",
    "def hot_shot(logits):\n",
    "    \n",
    "    \n",
    "    for i in range(1024):\n",
    "        \n",
    "        array = logits[i,:].detach().numpy()\n",
    "        max_idx = np.argmax(array)\n",
    "        array = np.zeros(19)\n",
    "        array[max_idx] = 1.0\n",
    "\n",
    "        if i==0:\n",
    "            hot_shots = T.tensor(array, dtype=T.float)\n",
    "        else:   \n",
    "            hot_shots = T.vstack((hot_shots,T.tensor(array, dtype=T.float)))\n",
    "    \n",
    "    return hot_shots #return a tensor of the same size that was input but with a hot shot\n",
    "\n",
    "def gumbel_rumbel(logits):\n",
    "    #Takes as input a tensor with 1024 rows and 19 columns and returns a a tensor with same shape with gumbel max per row\n",
    "    for i in range(1024):\n",
    "\n",
    "        row = logits[i,:]\n",
    "        row = F.gumbel_softmax(row, hard=True)\n",
    "    \n",
    "\n",
    "        if i==0:\n",
    "            gumbels = row\n",
    "        else:   \n",
    "            gumbels = T.vstack((gumbels,row))\n",
    "\n",
    "    return gumbels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b78095b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodes: 0 avg win rate: 0.0\n",
      "episodes: 100 avg win rate: 0.0\n",
      "episodes: 200 avg win rate: 0.0\n",
      "episodes: 300 avg win rate: 0.0\n",
      "episodes: 400 avg win rate: 0.0\n",
      "episodes: 500 avg win rate: 0.0\n",
      "episodes: 600 avg win rate: 0.0\n",
      "episodes: 700 avg win rate: 0.0\n",
      "episodes: 800 avg win rate: 0.0\n",
      "episodes: 900 avg win rate: 0.0\n",
      "episodes: 1000 avg win rate: 0.0\n",
      "episodes: 1100 avg win rate: 0.0\n",
      "episodes: 1200 avg win rate: 0.0\n",
      "episodes: 1300 avg win rate: 0.0\n",
      "episodes: 1400 avg win rate: 0.0\n",
      "episodes: 1500 avg win rate: 0.0\n",
      "episodes: 1600 avg win rate: 0.0\n",
      "episodes: 1700 avg win rate: 0.0\n",
      "episodes: 1800 avg win rate: 0.0\n",
      "episodes: 1900 avg win rate: 0.0\n",
      "episodes: 2000 avg win rate: 0.0\n",
      "episodes: 2100 avg win rate: 0.0\n",
      "episodes: 2200 avg win rate: 0.0\n",
      "episodes: 2300 avg win rate: 0.0\n",
      "episodes: 2400 avg win rate: 0.0\n",
      "episodes: 2500 avg win rate: 0.0\n",
      "episodes: 2600 avg win rate: 0.0\n",
      "episodes: 2700 avg win rate: 0.0\n",
      "episodes: 2800 avg win rate: 0.0\n",
      "episodes: 2900 avg win rate: 0.0\n",
      "episodes: 3000 avg win rate: 0.0\n",
      "episodes: 3100 avg win rate: 0.0\n",
      "episodes: 3200 avg win rate: 0.0\n",
      "episodes: 3300 avg win rate: 0.0\n",
      "episodes: 3400 avg win rate: 0.0\n",
      "episodes: 3500 avg win rate: 0.0\n",
      "episodes: 3600 avg win rate: 0.0\n",
      "episodes: 3700 avg win rate: 0.0\n",
      "episodes: 3800 avg win rate: 0.0\n",
      "episodes: 3900 avg win rate: 0.0\n",
      "episodes: 4000 avg win rate: 0.0\n",
      "episodes: 4100 avg win rate: 0.0\n",
      "episodes: 4200 avg win rate: 0.0\n",
      "episodes: 4300 avg win rate: 0.0\n",
      "episodes: 4400 avg win rate: 0.0\n",
      "episodes: 4500 avg win rate: 0.0\n",
      "episodes: 4600 avg win rate: 0.0\n",
      "episodes: 4700 avg win rate: 0.0\n",
      "episodes: 4800 avg win rate: 0.0\n",
      "episodes: 4900 avg win rate: 0.0\n",
      "episodes: 5000 avg win rate: 0.0\n",
      "episodes: 5100 avg win rate: 0.0\n",
      "episodes: 5200 avg win rate: 0.0\n",
      "episodes: 5300 avg win rate: 0.0\n",
      "episodes: 5400 avg win rate: 0.0\n",
      "episodes: 5500 avg win rate: 0.0\n",
      "episodes: 5600 avg win rate: 0.0\n",
      "episodes: 5700 avg win rate: 0.0\n",
      "episodes: 5800 avg win rate: 0.0\n",
      "episodes: 5900 avg win rate: 0.0\n",
      "episodes: 6000 avg win rate: 0.0\n",
      "episodes: 6100 avg win rate: 0.0\n",
      "episodes: 6200 avg win rate: 0.0\n",
      "episodes: 6300 avg win rate: 0.0\n",
      "episodes: 6400 avg win rate: 0.0\n",
      "episodes: 6500 avg win rate: 0.0\n",
      "episodes: 6600 avg win rate: 0.0\n",
      "episodes: 6700 avg win rate: 0.0\n",
      "episodes: 6800 avg win rate: 0.0\n",
      "episodes: 6900 avg win rate: 0.0\n",
      "episodes: 7000 avg win rate: 0.0\n",
      "episodes: 7100 avg win rate: 0.0\n",
      "episodes: 7200 avg win rate: 0.0\n",
      "episodes: 7300 avg win rate: 0.0\n",
      "episodes: 7400 avg win rate: 0.0\n",
      "episodes: 7500 avg win rate: 0.0\n",
      "episodes: 7600 avg win rate: 0.0\n",
      "episodes: 7700 avg win rate: 0.0\n",
      "episodes: 7800 avg win rate: 0.0\n",
      "episodes: 7900 avg win rate: 0.0\n",
      "episodes: 8000 avg win rate: 0.0\n",
      "episodes: 8100 avg win rate: 0.0\n",
      "episodes: 8200 avg win rate: 0.0\n",
      "episodes: 8300 avg win rate: 0.0\n",
      "episodes: 8400 avg win rate: 0.0\n",
      "episodes: 8500 avg win rate: 0.0\n",
      "episodes: 8600 avg win rate: 0.0\n",
      "episodes: 8700 avg win rate: 0.0\n",
      "episodes: 8800 avg win rate: 0.0\n",
      "episodes: 8900 avg win rate: 0.0\n",
      "episodes: 9000 avg win rate: 0.0\n",
      "episodes: 9100 avg win rate: 0.0\n",
      "episodes: 9200 avg win rate: 0.0\n",
      "episodes: 9300 avg win rate: 0.0\n",
      "episodes: 9400 avg win rate: 0.0\n",
      "episodes: 9500 avg win rate: 0.0\n",
      "episodes: 9600 avg win rate: 0.0\n",
      "episodes: 9700 avg win rate: 0.0\n",
      "episodes: 9800 avg win rate: 0.0\n",
      "episodes: 9900 avg win rate: 0.0\n",
      "total duration (mins): 680.6742439905803\n"
     ]
    }
   ],
   "source": [
    "#Finally, with all classes and methods defined, create the script\n",
    "#to run MADDPG learning\n",
    "\n",
    "env_name = '3_vs_3_auto_GK'\n",
    "env = ft.RllibGFootball(env_name,write_video=False, render=False)\n",
    "\n",
    "#Initialize the players and replay buffer\n",
    "players = MADDPG_learning(2,1024)\n",
    "experience_replay = Replay_Buffer(1000000)\n",
    "\n",
    "beginning_time = time.time()\n",
    "\n",
    "n_training_episodes = 10000 #How many episodes to run the training for\n",
    "# score_tracker = [] #Capture the end of episode score every couple episodes\n",
    "total_steps = 0\n",
    "wins = np.zeros(n_training_episodes)\n",
    "avg_win_rate = []\n",
    "\n",
    "for i in range(n_training_episodes):\n",
    "    #initialize the GFootball environment\n",
    "    score = 0\n",
    "    obs = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        actions = players.choosing_actions(obs)\n",
    "        player_0_act = int(np.where(actions[0]==1)[0])\n",
    "        player_1_act = int(np.where(actions[1]==1)[0])\n",
    "        \n",
    "        actions = {'player_0':player_0_act,'player_1':player_1_act}\n",
    "        \n",
    "        next_obs, rewards, dones, infos = env.step(actions)\n",
    "        player_0_obs = obs['player_0']\n",
    "        player_1_obs = obs['player_1']\n",
    "        player_0_r = rewards['player_0']\n",
    "        player_1_r = rewards['player_1']\n",
    "        player_0_action = np.zeros(19)\n",
    "        player_1_action = np.zeros(19)\n",
    "        player_0_action[player_0_act] = 1.0\n",
    "        player_1_action[player_1_act] = 1.0\n",
    "        player_0_next_obs = next_obs['player_0']\n",
    "        player_1_next_obs = next_obs['player_1']\n",
    "        terms = dones['__all__']\n",
    "        \n",
    "        \n",
    "        \n",
    "        experience_replay.new_experience(player_0_obs, player_1_obs, player_0_r, player_1_r, player_0_action, \\\n",
    "                       player_1_action, player_0_next_obs, player_1_next_obs,terms)\n",
    "        \n",
    "\n",
    "        if total_steps % 40 == 0:\n",
    "            if len(experience_replay.replay_buffer) > 1024:\n",
    "                players.MDDPG_learn(experience_replay)\n",
    "                \n",
    "        \n",
    "        if i % 100 == 0: #Every 100 episodes, get the average win rate over the past 100 episodes\n",
    "            if infos['player_1']['game_info']['steps_left'] == 500:\n",
    "                avg_win_rate.append(wins[-99:-1].mean())\n",
    "                print('episodes: {}'.format(i),'avg win rate: {}'.format(wins[-99:-1].mean()))\n",
    "            \n",
    "        \n",
    "        obs = next_obs\n",
    "#         score_tracker.append()\n",
    "        total_steps+=1\n",
    "        \n",
    "    \n",
    "        if dones['__all__']==True:\n",
    "            \n",
    "            game_result = 0 if infos['player_0']['score_reward'] == -1 else \\\n",
    "            1 if infos['player_0']['score_reward'] == 1 else 0\n",
    "            \n",
    "            wins[i] = game_result\n",
    "            \n",
    "            done=True\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "ending_time = time.time() \n",
    "    \n",
    "print('total duration (mins): {}'.format((ending_time-beginning_time)/60))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9894fba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the models\n",
    "for player_idx, player in enumerate(players.players):\n",
    "    T.save(player.Actor_trainer.state_dict(), '/mnt/rldm/notebooks/model_{}_v4.pth'.format(player_idx))\n",
    "\n",
    "# # load the model for player 0\n",
    "player_0 = Player(0.95,0.01,19)\n",
    "player_0.Actor_trainer.load_state_dict(T.load('/mnt/rldm/notebooks/model_{}_v4.pth'.format(0)))\n",
    "\n",
    "# #load the model for player 1\n",
    "player_1 = Player(0.95,0.01,19)\n",
    "player_1.Actor_trainer.load_state_dict(T.load('/mnt/rldm/notebooks/model_{}_v4.pth'.format(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2239810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the trained agents using just the Actor_trainer for each player\n",
    "\n",
    "#Finally, with all classes and methods defined, create the script\n",
    "#to run MADDPG learning\n",
    "\n",
    "env_name = '3_vs_3_auto_GK'\n",
    "env = ft.RllibGFootball(env_name,write_video=False, render=False)\n",
    "\n",
    "\n",
    "beginning_time = time.time()\n",
    "\n",
    "n_training_episodes = 1 #How many episodes to run the training for\n",
    "# score_tracker = [] #Capture the end of episode score every couple episodes\n",
    "total_steps = 0\n",
    "wins = np.zeros(n_training_episodes)\n",
    "avg_win_rate = []\n",
    "\n",
    "#Metrics for activity and distance from each other\n",
    "player_0_active_cache = np.zeros((n_training_episodes,1000))\n",
    "player_1_active_cache = np.zeros((n_training_episodes,1000))\n",
    "distance = np.zeros((n_training_episodes,1000))\n",
    "ball_owner = np.zeros((n_training_episodes,1000))\n",
    "\n",
    "for i in range(n_training_episodes):\n",
    "    #initialize the GFootball environment\n",
    "    score = 0\n",
    "    obs = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    counter = 0\n",
    "    while not done:\n",
    "        action_0 = player_0.choose_action(obs['player_0'])\n",
    "        action_1 = player_1.choose_action(obs['player_1'])\n",
    "        player_0_act = int(np.where(action_0==1)[0])\n",
    "        player_1_act = int(np.where(action_1==1)[0])\n",
    "        \n",
    "        actions = {'player_0':player_0_act,'player_1':player_1_act}\n",
    "        \n",
    "        next_obs, rewards, dones, infos = env.step(actions)\n",
    "        player_0_obs = obs['player_0']\n",
    "        player_1_obs = obs['player_1']\n",
    "        player_0_r = rewards['player_0']\n",
    "        player_1_r = rewards['player_1']\n",
    "        player_0_action = np.zeros(19)\n",
    "        player_1_action = np.zeros(19)\n",
    "        player_0_action[player_0_act] = 1.0\n",
    "        player_1_action[player_1_act] = 1.0\n",
    "        player_0_next_obs = next_obs['player_0']\n",
    "        player_1_next_obs = next_obs['player_1']\n",
    "        terms = dones['__all__']\n",
    "        \n",
    "\n",
    "        player_0_active_cache[i,counter] = obs['player_0'][34]\n",
    "        player_1_active_cache[i,counter] = obs['player_0'][35]\n",
    "        distance[i,counter] = np.linalg.norm(obs['player_0'][2:4] - obs['player_0'][4:6])\n",
    "        ball_owner[i,counter] = infos['player_0']['game_info']['ball_owned_player']\n",
    "        \n",
    "        \n",
    "#         experience_replay.new_experience(player_0_obs, player_1_obs, player_0_r, player_1_r, player_0_action, \\\n",
    "#                        player_1_action, player_0_next_obs, player_1_next_obs,terms)\n",
    "        \n",
    "\n",
    "#         if total_steps % 100 == 0:\n",
    "#             if len(experience_replay.replay_buffer) > 1024:\n",
    "#                 players.MDDPG_learn(experience_replay)\n",
    "                \n",
    "        \n",
    "#         if i % 100 == 0: #Every 100 episodes, get the average win rate over the past 100 episodes\n",
    "#             if infos['player_1']['game_info']['steps_left'] == 500:\n",
    "#                 avg_win_rate.append(wins[-99:-1].mean())\n",
    "#                 print('episodes: {}'.format(i),'avg win rate: {}'.format(wins[-99:-1].mean()))\n",
    "            \n",
    "        \n",
    "        obs = next_obs\n",
    "#         score_tracker.append()\n",
    "        total_steps += 1\n",
    "        counter += 1\n",
    "    \n",
    "        if dones['__all__']==True:\n",
    "            \n",
    "            game_result = 0 if infos['player_0']['score_reward'] == -1 else \\\n",
    "            1 if infos['player_0']['score_reward'] == 1 else 0\n",
    "            \n",
    "            wins[i] = game_result\n",
    "            \n",
    "            done=True\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "ending_time = time.time() \n",
    "    \n",
    "# print('total duration (mins): {}'.format((ending_time-beginning_time)/60))\n",
    "    \n",
    "\n",
    "# data = gt.get_gif_html(videos_path=\"/tmp/football/episode_done_*.avi\", \n",
    "#                        title=\"Full episodes\")\n",
    "# HTML(data=data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4370ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this block of code to get the metrics that were created from the previous testing cell\n",
    "\n",
    "# # #Data from the test and save in csv\n",
    "distance_n0_v1 = np.delete(distance, np.argwhere(np.all(distance[..., :] == 0, axis=0)), axis=1)\n",
    "\n",
    "\n",
    "# # #Aggregation for player 0 and 1 distance from each other \n",
    "avg_distance_in_episode_v1 = np.mean(distance_n0_v1,axis=1)\n",
    "\n",
    "# #Aggregation data for owns the ball\n",
    "ball_owner = pd.DataFrame(ball_owner)\n",
    "ball_owner_agg =pd.DataFrame()\n",
    "ball_owner_agg['player_0'] = (ball_owner==1).sum(axis=1)\n",
    "ball_owner_agg['player_1'] = (ball_owner==2).sum(axis=1)\n",
    "\n",
    "\n",
    "## Sent it to CSV\n",
    "pd.DataFrame(avg_distance_in_episode_v1).to_csv('avg_distance_in_episode_v1.csv')\n",
    "ball_owner_agg.to_csv('ball_owner_v1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
